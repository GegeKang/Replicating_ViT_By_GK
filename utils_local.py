# utils.py - supporting functions for Vision Transformer
"""08_pytorch_paper_replicating/.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16bx6HSGbgh3DhF1C3h-NFI5T_StkjhQe
"""
try:
    import torch
    import torchvision
    assert int(torch.__version__.split('.')[1]) >= 12 or int(torch.__version__.split('.')[0]) == 2, 'torch version should be 1.12+'
    assert int(torchvision.__version__.split('.')[1]) >= 13, 'torchvision version should be 0.13+'
    print(f'torch version: {torch.__version__}')
    print(f'torchvision version: {torchvision.__version__}')
except:
    print(f'[INFO] torch/torchvision versions not as required, installing nightly versions.')
    import torch
    import torchvision
    print(f'torch version: {torch.__version__}')
    print(f'torchvision version: {torchvision.__version__}')
import matplotlib.pyplot as plt
import torch
import torchvision
from torch import nn
from torchvision import transforms
try:
    from torchinfo import summary
except:
    print("[INFO] Couldn't find torchinfo... installing it.")
    from torchinfo import summary
try:
    from going_modular.going_modular import data_setup, engine
    from helper_functions import download_data, set_seeds, plot_loss_curves
except:
    print("[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.")
    from going_modular.going_modular import data_setup, engine
    from helper_functions import download_data, set_seeds, plot_loss_curves
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device
image_path = download_data(source='https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip', destination='pizza_steak_sushi')
image_path
train_dir = image_path / 'train'
test_dir = image_path / 'test'
IMG_SIZE = 224
manual_transforms = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])
print(f'Manually created transforms: {manual_transforms}')
BATCH_SIZE = 32
train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir, test_dir=test_dir, transform=manual_transforms, batch_size=BATCH_SIZE)
(train_dataloader, test_dataloader, class_names)
image_batch, label_batch = next(iter(train_dataloader))
image, label = (image_batch[0], label_batch[0])
(image.shape, label)
plt.imshow(image.permute(1, 2, 0))
plt.title(class_names[label])
plt.axis(False)
height = 224
width = 224
color_channels = 3
patch_size = 16
number_of_patches = int(height * width / patch_size ** 2)
print(f'Number of patches (N) with image height (H={height}), width (W={width}) and patch size (P={patch_size}):{number_of_patches}')
embedding_layer_input_shape = (height, width, color_channels)
embedding_layer_output_shape = (number_of_patches, patch_size ** 2 * color_channels)
print(f'Input shape (single 2D image): {embedding_layer_input_shape}')
print(f'Output shape (single 2D image flattened into patches): {embedding_layer_output_shape}')
plt.imshow(image.permute(1, 2, 0))
plt.title(class_names[label])
plt.axis(False)
image_permuted = image.permute(1, 2, 0)
patch_size = 16
plt.figure(figsize=(patch_size, patch_size))
plt.imshow(image_permuted[:patch_size, :, :])
img_size = 224
patch_size = 16
num_patches = img_size / patch_size
assert img_size % patch_size == 0, 'Image size must be divisible by patch size'
print(f'Number of patches per row: {num_patches}\nPatch size: {patch_size} pixels x {patch_size} pixels')
fig, axs = plt.subplots(nrows=1, ncols=img_size // patch_size, figsize=(num_patches, num_patches), sharex=True, sharey=True)
for i, patch in enumerate(range(0, img_size, patch_size)):
    axs[i].imshow(image_permuted[:patch_size, patch:patch + patch_size, :])
    axs[i].set_xlabel(i + 1)
    axs[i].set_xticks([])
    axs[i].set_yticks([])
img_size = 224
patch_size = 16
num_patches = img_size / patch_size
assert img_size % patch_size == 0, 'Image size must be divisible by patch size'
print(f'Number of patches per row: {num_patches}        \nNumber of patches per column: {num_patches}        \nTotal patches: {num_patches * num_patches}        \nPatch size: {patch_size} pixels x {patch_size} pixels')
fig, axs = plt.subplots(nrows=img_size // patch_size, ncols=img_size // patch_size, figsize=(num_patches, num_patches), sharex=True, sharey=True)
for i, patch_height in enumerate(range(0, img_size, patch_size)):
    for j, patch_width in enumerate(range(0, img_size, patch_size)):
        axs[i, j].imshow(image_permuted[patch_height:patch_height + patch_size, patch_width:patch_width + patch_size, :])
        axs[i, j].set_ylabel(i + 1, rotation='horizontal', horizontalalignment='right', verticalalignment='center')
        axs[i, j].set_xlabel(j + 1)
        axs[i, j].set_xticks([])
        axs[i, j].set_yticks([])
        axs[i, j].label_outer()
fig.suptitle(f'{class_names[label]} -> Patchified', fontsize=16)
plt.show()
from torch import nn
patch_size = 16
conv2d = nn.Conv2d(in_channels=3, out_channels=768, kernel_size=patch_size, stride=patch_size, padding=0)
plt.imshow(image.permute(1, 2, 0))
plt.title(class_names[label])
plt.axis(False)
image_out_of_conv = conv2d(image.unsqueeze(0))
print(image_out_of_conv.shape)
import random
random_indexes = random.sample(range(0, 758), k=5)
print(f'Showing random convolutional feature maps from indexes: {random_indexes}')
fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12, 12))
for i, idx in enumerate(random_indexes):
    image_conv_feature_map = image_out_of_conv[:, idx, :, :]
    axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())
    axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])
single_feature_map = image_out_of_conv[:, 0, :, :]
(single_feature_map, single_feature_map.requires_grad)
flatten = nn.Flatten(start_dim=2, end_dim=3)
plt.imshow(image.permute(1, 2, 0))
plt.title(class_names[label])
plt.axis(False)
print(f'Original image shape: {image.shape}')
image_out_of_conv = conv2d(image.unsqueeze(0))
print(f'Image feature map shape: {image_out_of_conv.shape}')
image_out_of_conv_flattened = flatten(image_out_of_conv)
print(f'Flattened image feature map shape: {image_out_of_conv_flattened.shape}')
image_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0, 2, 1)
print(f'Patch embedding sequence shape: {image_out_of_conv_flattened_reshaped.shape} -> [batch_size, num_patches, embedding_size]')
single_flattened_feature_map = image_out_of_conv_flattened_reshaped[:, :, 0]
plt.figure(figsize=(22, 22))
plt.imshow(single_flattened_feature_map.detach().numpy())
plt.title(f'Flattened feature map shape: {single_flattened_feature_map.shape}')
plt.axis(False)
(single_flattened_feature_map, single_flattened_feature_map.requires_grad, single_flattened_feature_map.shape)
set_seeds()
patchify = PatchEmbedding(in_channels=3, patch_size=16, embedding_dim=768)
print(f'Input image shape: {image.unsqueeze(0).shape}')
patch_embedded_image = patchify(image.unsqueeze(0))
print(f'Output patch embedding shape: {patch_embedded_image.shape}')
batch_size = patch_embedded_image.shape[0]
embedding_dimension = patch_embedded_image.shape[-1]
class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), requires_grad=True)
print(class_token[:, :, :10])
print(f'Class token shape: {class_token.shape} -> [batch_size, number_of_tokens, embedding_dimension]')
patch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image), dim=1)
print(patch_embedded_image_with_class_embedding)
print(f'Sequence of patch embeddings with class token prepended shape: {patch_embedded_image_with_class_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]')
(patch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape)
number_of_patches = int(height * width / patch_size ** 2)
embedding_dimension = patch_embedded_image_with_class_embedding.shape[2]
position_embedding = nn.Parameter(torch.ones(1, number_of_patches + 1, embedding_dimension), requires_grad=True)
print(position_embedding[:, :10, :10])
print(f'Position embedding shape: {position_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]')
patch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding
print(patch_and_position_embedding)
print(f'Patch embeddings, class token prepended and positional embeddings added shape: {patch_and_position_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]')
set_seeds()
patch_size = 16
print(f'Image tensor shape: {image.shape}')
height, width = (image.shape[1], image.shape[2])
x = image.unsqueeze(0)
print(f'Input image with batch dimension shape: {x.shape}')
patch_embedding_layer = PatchEmbedding(in_channels=3, patch_size=patch_size, embedding_dim=768)
patch_embedding = patch_embedding_layer(x)
print(f'Patching embedding shape: {patch_embedding.shape}')
batch_size = patch_embedding.shape[0]
embedding_dimension = patch_embedding.shape[-1]
class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), requires_grad=True)
print(f'Class token embedding shape: {class_token.shape}')
patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)
print(f'Patch embedding with class token shape: {patch_embedding_class_token.shape}')
number_of_patches = int(height * width / patch_size ** 2)
position_embedding = nn.Parameter(torch.ones(1, number_of_patches + 1, embedding_dimension), requires_grad=True)
patch_and_position_embedding = patch_embedding_class_token + position_embedding
print(f'Patch and position embedding shape: {patch_and_position_embedding.shape}')
multihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=768, num_heads=12)
patched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)
print(f'Input shape of MSA block: {patch_and_position_embedding.shape}')
print(f'Output shape MSA block: {patched_image_through_msa_block.shape}')
mlp_block = MLPBlock(embedding_dim=768, mlp_size=3072, dropout=0.1)
patched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)
print(f'Input shape of MLP block: {patched_image_through_msa_block.shape}')
print(f'Output shape MLP block: {patched_image_through_mlp_block.shape}')
torch_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768, nhead=12, dim_feedforward=3072, dropout=0.1, activation='gelu', batch_first=True, norm_first=True)
torch_transformer_encoder_layer
batch_size = 32
class_token_embedding_single = nn.Parameter(data=torch.randn(1, 1, 768))
class_token_embedding_expanded = class_token_embedding_single.expand(batch_size, -1, -1)
print(f'Shape of class token embedding single: {class_token_embedding_single.shape}')
print(f'Shape of class token embedding expanded: {class_token_embedding_expanded.shape}')
set_seeds()
random_image_tensor = torch.randn(1, 3, 224, 224)
vit = ViT(num_classes=len(class_names))
vit(random_image_tensor)
from going_modular.going_modular import engine
optimizer = torch.optim.Adam(params=vit.parameters(), lr=0.003, betas=(0.9, 0.999), weight_decay=0.3)
loss_fn = torch.nn.CrossEntropyLoss()
set_seeds()
results = engine.train(model=vit, train_dataloader=train_dataloader, test_dataloader=test_dataloader, optimizer=optimizer, loss_fn=loss_fn, epochs=10, device=device)
from helper_functions import plot_loss_curves
plot_loss_curves(results)